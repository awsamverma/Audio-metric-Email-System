{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f0c38aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import mysql.connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d53b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = mysql.connector.connect(\n",
    "            host='localhost',  # Replace with your MySQL host\n",
    "            user='root',  # Replace with your MySQL username\n",
    "            password='Sam@9818',  # Replace with your MySQL password\n",
    "            database='mail_db' ,\n",
    "            port= '3306'# Replace with your MySQL database name\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc70746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2730468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b90f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define a function to extract MFCC features from an audio file\n",
    "def extract_mfcc_features(audio_file):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(audio_file)\n",
    "    \n",
    "    # Extract MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "    \n",
    "    return mfccs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc4ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "116f969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Replace the file paths with the actual paths to your audio recordings\n",
    "enrollment1_features = extract_mfcc_features(\"D:\\\\python dsa\\\\audio_1.wav\")\n",
    "enrollment2_features = extract_mfcc_features(\"D:\\\\python dsa\\\\audio_2.wav\")\n",
    "enrollment3_features = extract_mfcc_features(\"D:\\\\python dsa\\\\audio_3.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10da267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MFCC features for the test audio sample\n",
    "test_features = extract_mfcc_features(\"C:\\\\Users\\\\98117\\\\test.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7cfd034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare the Data\n",
    "# Organize your data into training and test sets\n",
    "\n",
    "# Find the maximum number of time steps among all speakers\n",
    "max_time_steps = max(enrollment1_features.shape[1], enrollment2_features.shape[1], enrollment3_features.shape[1])\n",
    "\n",
    "# Pad or truncate the MFCC features to have the same number of time steps\n",
    "enrollment1_features = np.pad(enrollment1_features, ((0, 0), (0, max_time_steps - enrollment1_features.shape[1])), mode='constant')\n",
    "enrollment2_features = np.pad(enrollment2_features, ((0, 0), (0, max_time_steps - enrollment2_features.shape[1])), mode='constant')\n",
    "enrollment3_features = np.pad(enrollment3_features, ((0, 0), (0, max_time_steps - enrollment3_features.shape[1])), mode='constant')\n",
    "test_features = np.pad(test_features, ((0, 0), (0, max_time_steps - test_features.shape[1])), mode='constant')\n",
    "\n",
    "# Create NumPy arrays from the padded/truncated features\n",
    "X_train = np.array([enrollment1_features, enrollment2_features, enrollment3_features])\n",
    "y_train = np.array([0, 1, 2])  # Assign unique labels to each speaker (0, 1, 2 in this example)\n",
    "X_test = np.array([test_features])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc49cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define the CNN Model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    layers.Conv1D(64, kernel_size=3, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(np.unique(y_train)), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddf3c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c79ced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 54.7927 - accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 55.6828 - accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 55.2519 - accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 26.8953 - accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.1816 - accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.8578 - accuracy: 0.6667\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19ddc199ad0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d812578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 164ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Verification\n",
    "# Predict the label for the test audio sample\n",
    "predicted_label = np.argmax(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f02b6762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 2 verified.\n"
     ]
    }
   ],
   "source": [
    "# Map the predicted label to the corresponding speaker\n",
    "speakers = {0: \"Speaker 1\", 1: \"Speaker 2\", 2: \"Speaker 3\"}\n",
    "if predicted_label in speakers:\n",
    "    print(f\"{speakers[predicted_label]} verified.\")\n",
    "else:\n",
    "    print(\"Speaker not recognized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b428fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
